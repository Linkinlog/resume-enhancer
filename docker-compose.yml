services:
  caddy:
    build:
      context: .
      dockerfile: caddy.Dockerfile
    ports:
      - "80:80"
    networks:
      - resume-network
    depends_on:
      - llama
      - backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    networks:
      - resume-network
    depends_on:
      - llama
  llama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - resume-network
    entrypoint: /bin/bash
    command: -c  "ollama serve & sleep 2 && ollama run llama3.2:1b-instruct-q3_K_S && wait"
networks:
  resume-network:
    driver: bridge
volumes:
  ollama:
    driver: local
