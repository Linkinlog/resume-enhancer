services:
  caddy:
    build:
      context: .
      dockerfile: caddy.Dockerfile
    ports:
      - "1223:80"
    networks:
      - resume-network
    depends_on:
      - llama
      - backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    networks:
      - resume-network
    depends_on:
      - llama
  llama:
    image: ollama/ollama
    volumes:
      - ollama:/root/.ollama
      - ./Modelfile:/root/.ollama/Modelfile
    networks:
      - resume-network
    entrypoint: /bin/bash
    command: -c  "ollama serve & \
                  sleep 2 && \
                  ollama create resume -f /root/.ollama/Modelfile && \
                  ollama run resume && \
                  wait"
networks:
  resume-network:
    driver: bridge
volumes:
  ollama:
    driver: local
